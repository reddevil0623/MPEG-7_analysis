{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 420 out of 420 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done 162 tasks      | elapsed:   16.7s\n",
      "[Parallel(n_jobs=-1)]: Done 288 tasks      | elapsed:   33.0s\n",
      "[Parallel(n_jobs=-1)]: Done 420 out of 420 | elapsed:   49.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target length for padding: 1379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed: 36.3min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed: 108.8min\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed: 229.1min\n",
      "[Parallel(n_jobs=-1)]: Done 420 out of 420 | elapsed: 381.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy over 100 runs: 0.9167 ± 0.0144\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.ndimage import label\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import cdist\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Make sure to adjust this path so Python can find your eucalc package\n",
    "sys.path.append('eucalc_directory')\n",
    "import eucalc as ec\n",
    "\n",
    "# ─── Settings ─────────────────────────────────────────────────────────────────\n",
    "datafolder    = \".\"       # directory containing your .gif files\n",
    "k             = 360       # number of random directions for ECT\n",
    "xinterval     = (-1.5, 1.5)\n",
    "xpoints       = 12000\n",
    "padding_extra = 10        # extra pixels around max dimension after crop\n",
    "keywords      = [\n",
    "    'apple', 'bell', 'bottle', 'car', 'classic',\n",
    "    'cup', 'device', 'face', 'heart', 'key'\n",
    "]\n",
    "n_repeats     = 100       # how many train/test splits to average over\n",
    "test_frac     = 0.30      # fraction for test set (70/30 split)\n",
    "\n",
    "# ─── Preprocessing Functions ──────────────────────────────────────────────────\n",
    "def filter_to_largest_cc(img_array):\n",
    "    \"\"\"Return binary mask of the largest connected white component.\"\"\"\n",
    "    mask = img_array > 0\n",
    "    labeled, num = label(mask, structure=np.ones((3,3)))\n",
    "    if num < 1:\n",
    "        return np.zeros_like(img_array, dtype=np.uint8)\n",
    "    sizes = np.bincount(labeled.ravel())\n",
    "    largest = np.argmax(sizes[1:]) + 1\n",
    "    return (labeled == largest).astype(np.uint8)\n",
    "\n",
    "\n",
    "def normalize_cc_size(mask, target_area):\n",
    "    \"\"\"Scale mask so its white-pixel area ≈ target_area.\"\"\"\n",
    "    current = mask.sum()\n",
    "    if current in (0, target_area):\n",
    "        return mask\n",
    "    scale = np.sqrt(target_area / current)\n",
    "    h, w = mask.shape\n",
    "    new_h = max(1, int(round(h * scale)))\n",
    "    new_w = max(1, int(round(w * scale)))\n",
    "    pil = Image.fromarray((mask * 255).astype(np.uint8))\n",
    "    resized = pil.resize((new_w, new_h), resample=Image.BILINEAR)\n",
    "    return (np.array(resized) > 127).astype(np.uint8)\n",
    "\n",
    "\n",
    "def crop_to_shape(mask):\n",
    "    \"\"\"Crop mask tightly around its white pixels.\"\"\"\n",
    "    coords = np.argwhere(mask > 0)\n",
    "    if coords.size == 0:\n",
    "        return mask\n",
    "    r0, c0 = coords.min(axis=0)\n",
    "    r1, c1 = coords.max(axis=0)\n",
    "    return mask[r0:r1+1, c0:c1+1]\n",
    "\n",
    "\n",
    "def pad_image_to_square(img, target_length):\n",
    "    \"\"\"Pad image to a centered square of side target_length.\"\"\"\n",
    "    h, w = img.shape\n",
    "    pad_h = (target_length - h) // 2\n",
    "    pad_w = (target_length - w) // 2\n",
    "    out = np.zeros((target_length, target_length), dtype=np.uint8)\n",
    "    out[pad_h:pad_h+h, pad_w:pad_w+w] = img\n",
    "    return out\n",
    "\n",
    "\n",
    "# ─── ECT & Distance Functions ────────────────────────────────────────────────\n",
    "class EctImg:\n",
    "    def __init__(self, nm, img, k=k, xinterval=xinterval, xpoints=xpoints):\n",
    "        self.nm = nm\n",
    "        self.image = self.compute(img, k, xinterval, xpoints)\n",
    "\n",
    "    def compute(self, img, k, xinterval, xpoints):\n",
    "        cplx = ec.EmbeddedComplex(img)\n",
    "        cplx.preproc_ect()\n",
    "        thetas = np.random.uniform(0, 2 * np.pi, k)\n",
    "        ect1 = np.empty((k, xpoints), dtype=float)\n",
    "        T = np.linspace(xinterval[0], xinterval[1], xpoints)\n",
    "        for i, theta in enumerate(thetas):\n",
    "            direction = np.array((np.sin(theta), np.cos(theta)))\n",
    "            ect_dir = cplx.compute_euler_characteristic_transform(direction)\n",
    "            ect1[i] = [ect_dir.evaluate(t) for t in T]\n",
    "        return ect1\n",
    "\n",
    "\n",
    "def wasserstein_distance(emp1, emp2, delta_x=1.0):\n",
    "    cost_matrix = cdist(emp1, emp2, metric='minkowski', p=1) * delta_x\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    return np.mean(cost_matrix[row_ind, col_ind])\n",
    "\n",
    "\n",
    "def compute_distance_matrix_wasserstein_parallel(ects, delta_x,\n",
    "                                                n_jobs=-1, verbose=5):\n",
    "    def compute_row(i):\n",
    "        dists = [\n",
    "            wasserstein_distance(ects[i].image, ects[j].image, delta_x)\n",
    "            for j in range(len(ects))\n",
    "        ]\n",
    "        return i, dists\n",
    "\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=verbose)(\n",
    "        delayed(compute_row)(i) for i in range(len(ects))\n",
    "    )\n",
    "    N = len(ects)\n",
    "    D = np.zeros((N, N), dtype=float)\n",
    "    for i, row in results:\n",
    "        D[i, :] = row\n",
    "    return D\n",
    "\n",
    "\n",
    "# ─── Main Pipeline ────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    # 1) Gather GIF files matching keywords\n",
    "    all_files = [f for f in os.listdir(datafolder) if f.lower().endswith('.gif')]\n",
    "    files = [f for f in all_files\n",
    "             if any(kw in f.lower() for kw in keywords)]\n",
    "    if not files:\n",
    "        raise RuntimeError(\"No matching GIFs found in datafolder.\")\n",
    "\n",
    "    # Labels: first keyword found in filename\n",
    "    labels = [\n",
    "        next(kw for kw in keywords if kw in f.lower())\n",
    "        for f in files\n",
    "    ]\n",
    "\n",
    "    # 2) CC-filter to masks\n",
    "    masks = Parallel(n_jobs=-1, verbose=5)(\n",
    "        delayed(lambda f: filter_to_largest_cc(\n",
    "            np.array(Image.open(os.path.join(datafolder, f))).mean(axis=2)\n",
    "            if Image.open(os.path.join(datafolder, f)).mode == 'RGB'\n",
    "            else np.array(Image.open(os.path.join(datafolder, f)))\n",
    "        ))(f) for f in files\n",
    "    )\n",
    "    areas = [m.sum() for m in masks]\n",
    "    target_area = max(areas)\n",
    "\n",
    "    # 3) Normalize & crop\n",
    "    cropped = Parallel(n_jobs=-1, verbose=5)(\n",
    "        delayed(lambda i: crop_to_shape(\n",
    "            normalize_cc_size(masks[i], target_area)\n",
    "        ))(i) for i in range(len(files))\n",
    "    )\n",
    "    cropped_dict = dict(zip(files, cropped))\n",
    "\n",
    "    # 4) Pad to square\n",
    "    dims = [max(cropped_dict[f].shape) for f in files]\n",
    "    target_length = max(dims) + padding_extra\n",
    "    print(f\"Target length for padding: {target_length}\")\n",
    "    padded_dict = {\n",
    "        f: pad_image_to_square(cropped_dict[f], target_length)\n",
    "        for f in files\n",
    "    }\n",
    "\n",
    "    # 5) Compute ECTs\n",
    "    ects = [EctImg(f, padded_dict[f]) for f in files]\n",
    "\n",
    "    # 6) Compute Wasserstein distance matrix\n",
    "    delta_x = (xinterval[1] - xinterval[0]) / (xpoints - 1)\n",
    "    D = compute_distance_matrix_wasserstein_parallel(\n",
    "        ects, delta_x, n_jobs=-1, verbose=5\n",
    "    )\n",
    "\n",
    "    # 7) Build RBF‐style kernel from D\n",
    "    γ = 1.0 / np.median(D[np.triu_indices_from(D, k=1)])\n",
    "    K = np.exp(-γ * D)\n",
    "\n",
    "    # 8) SVM classification with precomputed kernel\n",
    "    idx = np.arange(len(files))\n",
    "    accuracies = []\n",
    "    for _ in range(n_repeats):\n",
    "        train_idx, test_idx, y_train, y_test = train_test_split(\n",
    "            idx, labels,\n",
    "            test_size=test_frac,\n",
    "            stratify=labels\n",
    "        )\n",
    "\n",
    "        K_train = K[np.ix_(train_idx, train_idx)]\n",
    "        K_test  = K[np.ix_(test_idx,  train_idx)]\n",
    "\n",
    "        clf = SVC(kernel='precomputed')\n",
    "        clf.fit(K_train, y_train)\n",
    "        accuracies.append(clf.score(K_test, y_test))\n",
    "\n",
    "    mean_acc = np.mean(accuracies)\n",
    "    std_acc  = np.std(accuracies)\n",
    "    print(f\"SVM accuracy over {n_repeats} runs: \"\n",
    "          f\"{mean_acc:.4f} ± {std_acc:.4f}\")\n",
    "\n",
    "    return accuracies\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## based on contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 397 out of 420 | elapsed:    1.2s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 420 out of 420 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 243\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVM accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(accs)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mstd(accs)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 243\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 221\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# 6) distance matrix\u001b[39;00m\n\u001b[1;32m    220\u001b[0m delta_x \u001b[38;5;241m=\u001b[39m (xinterval[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m-\u001b[39mxinterval[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m/\u001b[39m(xpoints\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 221\u001b[0m D \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_distance_matrix_wasserstein_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# 7) kernel + SVM\u001b[39;00m\n\u001b[1;32m    225\u001b[0m gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39mmedian(D[np\u001b[38;5;241m.\u001b[39mtriu_indices_from(D,k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)])\n",
      "Cell \u001b[0;32mIn[5], line 166\u001b[0m, in \u001b[0;36mcompute_distance_matrix_wasserstein_parallel\u001b[0;34m(ects, delta_x, n_jobs, verbose)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrowfun\u001b[39m(i):\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m i, [wasserstein_distance(ects[i]\u001b[38;5;241m.\u001b[39mimage, ects[j]\u001b[38;5;241m.\u001b[39mimage, delta_x)\n\u001b[1;32m    165\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ects))]\n\u001b[0;32m--> 166\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrowfun\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mects\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ects)\n\u001b[1;32m    170\u001b[0m D \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((N,N), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.ndimage import label\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.spatial.distance import cdist\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from skimage import measure\n",
    "\n",
    "# Make sure to adjust this path so Python can find your eucalc & My_ECT packages\n",
    "sys.path.append('eucalc_directory')\n",
    "import eucalc as ec\n",
    "import My_ECT as detect\n",
    "\n",
    "from numba.typed import List as NBList\n",
    "import numba\n",
    "\n",
    "# ─── Settings ─────────────────────────────────────────────────────────────────\n",
    "datafolder    = \".\"       # directory containing your .gif files\n",
    "k             = 360       # random directions for ECT\n",
    "xinterval     = (-1.5, 1.5)\n",
    "xpoints       = 12000\n",
    "padding_extra = 10\n",
    "keywords      = ['apple','bell','bottle','car','classic',\n",
    "                 'cup','device','face','heart','key']\n",
    "n_repeats     = 100\n",
    "test_frac     = 0.30\n",
    "\n",
    "# ─── Preprocessing ────────────────────────────────────────────────────────────\n",
    "def filter_to_largest_cc(img_array):\n",
    "    mask = img_array > 0\n",
    "    labeled, num = label(mask, structure=np.ones((3,3)))\n",
    "    if num < 1:\n",
    "        return np.zeros_like(img_array, dtype=np.uint8)\n",
    "    sizes  = np.bincount(labeled.ravel())\n",
    "    largest = np.argmax(sizes[1:]) + 1\n",
    "    return (labeled == largest).astype(np.uint8)\n",
    "\n",
    "def normalize_cc_size(mask, target_area):\n",
    "    cur = mask.sum()\n",
    "    if cur in (0, target_area):\n",
    "        return mask\n",
    "    scale = np.sqrt(target_area / cur)\n",
    "    h,w   = mask.shape\n",
    "    new_h = max(1, int(round(h * scale)))\n",
    "    new_w = max(1, int(round(w * scale)))\n",
    "    pil   = Image.fromarray((mask*255).astype(np.uint8))\n",
    "    resized = pil.resize((new_w,new_h), resample=Image.BILINEAR)\n",
    "    return (np.array(resized)>127).astype(np.uint8)\n",
    "\n",
    "def crop_to_shape(mask):\n",
    "    coords = np.argwhere(mask>0)\n",
    "    if coords.size==0:\n",
    "        return mask\n",
    "    r0,c0 = coords.min(axis=0)\n",
    "    r1,c1 = coords.max(axis=0)\n",
    "    return mask[r0:r1+1, c0:c1+1]\n",
    "\n",
    "def pad_image_to_square(img, target_length):\n",
    "    h,w    = img.shape\n",
    "    pad_h  = (target_length-h)//2\n",
    "    pad_w  = (target_length-w)//2\n",
    "    out    = np.zeros((target_length,target_length),dtype=np.uint8)\n",
    "    out[pad_h:pad_h+h, pad_w:pad_w+w] = img\n",
    "    return out\n",
    "\n",
    "# ─── Contour → Simplicial Conversion ─────────────────────────────────────────\n",
    "def extract_boundary(mask):\n",
    "    return measure.find_contours(mask.astype(float), level=0.5)\n",
    "\n",
    "def contours_to_simplicial(contours):\n",
    "    all_pts, simplices = [], []\n",
    "    offset = 0\n",
    "    for loop in contours:\n",
    "        N = loop.shape[0]\n",
    "        all_pts.append(loop)\n",
    "        for j in range(N):\n",
    "            v1 = offset + j     + 1\n",
    "            v2 = offset + (j+1)%N + 1\n",
    "            simplices.append([v1, v2])\n",
    "        offset += N\n",
    "    data = np.vstack(all_pts)\n",
    "    return data, simplices\n",
    "\n",
    "# ─── Numba‐njitted ECT internals ──────────────────────────────────────────────\n",
    "@numba.njit()\n",
    "def euler_critical_values(simp_comp, data, direction):\n",
    "    n = len(simp_comp)\n",
    "    filt = np.empty((n,2),dtype=np.float64)\n",
    "    for i in range(n):\n",
    "        simplex = simp_comp[i]\n",
    "        fv = -1e18\n",
    "        for j in range(len(simplex)):\n",
    "            v  = simplex[j]\n",
    "            dv = data[v-1][0]*direction[0] + data[v-1][1]*direction[1]\n",
    "            if dv>fv: fv = dv\n",
    "        filt[i,0] = float(len(simplex))\n",
    "        filt[i,1] = fv\n",
    "    return filt\n",
    "\n",
    "@numba.njit()\n",
    "def euler_curve(simp_comp, data, direction, interval, points):\n",
    "    filt = euler_critical_values(simp_comp, data, direction)\n",
    "    idx  = np.argsort(filt[:,1])\n",
    "    filt = filt[idx]\n",
    "    step = (interval[1]-interval[0])/(points-1)\n",
    "    chi  = np.empty(points, dtype=np.float64)\n",
    "    val  = 0.0\n",
    "    c    = 0\n",
    "    for i in range(points):\n",
    "        x = interval[0] + i*step\n",
    "        while c<filt.shape[0] and filt[c,1]<=x+step:\n",
    "            dim = int(filt[c,0])\n",
    "            val += (-1)**(dim-1)\n",
    "            c   += 1\n",
    "        chi[i] = val\n",
    "    return chi\n",
    "\n",
    "# ─── Typed‐List Random ECT ───────────────────────────────────────────────────\n",
    "def RandomEct_2d(simp_comp, data, k=20, interval=(-1.,1.), points=100, factor=3):\n",
    "    # pack simplices\n",
    "    numba_simp = NBList()\n",
    "    for s in simp_comp:\n",
    "        numba_simp.append(NBList(s))\n",
    "    # pack data points\n",
    "    numba_data = NBList()\n",
    "    for pt in data:\n",
    "        numba_data.append(NBList((float(pt[0]), float(pt[1]))))\n",
    "\n",
    "    thetas = 2*np.pi*np.random.rand(k)\n",
    "    ect = np.empty((k, points), dtype=np.float64)\n",
    "    for i in numba.prange(k):\n",
    "        th = thetas[i]\n",
    "        dir_vec = np.array((np.sin(th), np.cos(th)), dtype=np.float64)\n",
    "        full = euler_curve(numba_simp, numba_data, dir_vec,\n",
    "                           interval, points*factor)\n",
    "        ect[i] = full[::factor]\n",
    "    return ect\n",
    "\n",
    "# ─── EctImg wrapper ──────────────────────────────────────────────────────────\n",
    "class EctImg:\n",
    "    def __init__(self, nm, simplices, data,\n",
    "                 k=k, interval=xinterval, points=xpoints):\n",
    "        self.nm    = nm\n",
    "        # call our typed‐list ECT\n",
    "        self.image = RandomEct_2d(simp_comp=simplices,\n",
    "                                  data=data,\n",
    "                                  k=k,\n",
    "                                  interval=interval,\n",
    "                                  points=points)\n",
    "\n",
    "# ─── Distance & Kernel & SVM ─────────────────────────────────────────────────\n",
    "def wasserstein_distance(emp1, emp2, delta_x=1.0):\n",
    "    C = cdist(emp1, emp2, metric='minkowski', p=1) * delta_x\n",
    "    r,c = linear_sum_assignment(C)\n",
    "    return np.mean(C[r,c])\n",
    "\n",
    "def compute_distance_matrix_wasserstein_parallel(ects, delta_x, n_jobs=-1, verbose=5):\n",
    "    def rowfun(i):\n",
    "        return i, [wasserstein_distance(ects[i].image, ects[j].image, delta_x)\n",
    "                  for j in range(len(ects))]\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=verbose)(\n",
    "        delayed(rowfun)(i) for i in range(len(ects))\n",
    "    )\n",
    "    N = len(ects)\n",
    "    D = np.zeros((N,N), dtype=float)\n",
    "    for i,row in results:\n",
    "        D[i,:] = row\n",
    "    return D\n",
    "\n",
    "# ─── Main Pipeline ────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    # 1) gather & filter GIFs\n",
    "    all_files = [f for f in os.listdir(datafolder) if f.lower().endswith('.gif')]\n",
    "    files     = [f for f in all_files if any(kw in f.lower() for kw in keywords)]\n",
    "    if not files:\n",
    "        raise RuntimeError(\"No matching GIFs found.\")\n",
    "\n",
    "    labels = [next(kw for kw in keywords if kw in f.lower()) for f in files]\n",
    "\n",
    "    # 2) CC‐filter\n",
    "    masks = Parallel(n_jobs=-1,verbose=5)(\n",
    "      delayed(lambda f: filter_to_largest_cc(\n",
    "          np.array(Image.open(os.path.join(datafolder,f))).mean(axis=2)\n",
    "          if Image.open(os.path.join(datafolder,f)).mode=='RGB'\n",
    "          else np.array(Image.open(os.path.join(datafolder,f)))\n",
    "      ))(f) for f in files\n",
    "    )\n",
    "    target_area = max(m.sum() for m in masks)\n",
    "\n",
    "    # 3) normalize, crop, pad\n",
    "    cropped = [crop_to_shape(normalize_cc_size(m, target_area)) for m in masks]\n",
    "    dims    = [max(c.shape) for c in cropped]\n",
    "    pad_len = max(dims) + padding_extra\n",
    "    padded_dict = {\n",
    "      files[i]: pad_image_to_square(cropped[i], pad_len)\n",
    "      for i in range(len(files))\n",
    "    }\n",
    "\n",
    "    # 4) extract contours → data/simplices\n",
    "    contour_dict = {}\n",
    "    for f,mask in padded_dict.items():\n",
    "        loops = extract_boundary(mask)\n",
    "        data, simplices = contours_to_simplicial(loops)\n",
    "        contour_dict[f] = {'data':data, 'simplices':simplices}\n",
    "\n",
    "    # 5) build EctImg objects\n",
    "    ects = [\n",
    "      EctImg(f,\n",
    "             contour_dict[f]['simplices'],\n",
    "             contour_dict[f]['data'])\n",
    "      for f in files\n",
    "    ]\n",
    "\n",
    "    # 6) distance matrix\n",
    "    delta_x = (xinterval[1]-xinterval[0])/(xpoints-1)\n",
    "    D = compute_distance_matrix_wasserstein_parallel(ects, delta_x,\n",
    "                                                     n_jobs=-1, verbose=5)\n",
    "\n",
    "    # 7) kernel + SVM\n",
    "    gamma =  1.0/np.median(D[np.triu_indices_from(D,k=1)])\n",
    "    K     = np.exp(-gamma * D)\n",
    "\n",
    "    idx    = np.arange(len(files))\n",
    "    accs   = []\n",
    "    for _ in range(n_repeats):\n",
    "        ti, te, ytr, yte = train_test_split(\n",
    "            idx, labels, test_size=test_frac, stratify=labels\n",
    "        )\n",
    "        Ktr = K[np.ix_(ti,ti)]\n",
    "        Kte = K[np.ix_(te,ti)]\n",
    "        clf = SVC(kernel='precomputed')\n",
    "        clf.fit(Ktr, ytr)\n",
    "        accs.append(clf.score(Kte, yte))\n",
    "\n",
    "    print(f\"SVM accuracy: {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECT metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 397 out of 420 | elapsed:    1.3s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 420 out of 420 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 420 out of 420 | elapsed:  6.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy over 100 runs: 0.8623 ± 0.0171\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.ndimage import label\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import eucalc as ec  # make sure 'eucalc_directory' is on PYTHONPATH\n",
    "\n",
    "# ─── Settings ─────────────────────────────────────────────────────────────────\n",
    "datafolder    = \".\"       # directory containing your .gif files\n",
    "k             = 360       # number of random directions for ECT\n",
    "xinterval     = (-1.5, 1.5)\n",
    "xpoints       = 12000     # sample points along each direction\n",
    "padding_extra = 10        # extra pixels when padding to square\n",
    "keywords      = [\n",
    "    'apple','bell','bottle','car','classic',\n",
    "    'cup','device','face','heart','key'\n",
    "]\n",
    "n_repeats     = 100       # how many train/test splits to average over\n",
    "test_frac     = 0.30      # fraction for test set (70/30 split)\n",
    "\n",
    "# ─── Preprocessing Functions ──────────────────────────────────────────────────\n",
    "def filter_to_largest_cc(img_array):\n",
    "    \"\"\"Return binary mask of the largest connected white component.\"\"\"\n",
    "    mask = img_array > 0\n",
    "    labeled, num = label(mask, structure=np.ones((3,3)))\n",
    "    if num < 1:\n",
    "        return np.zeros_like(img_array, dtype=np.uint8)\n",
    "    sizes  = np.bincount(labeled.ravel())\n",
    "    largest = np.argmax(sizes[1:]) + 1\n",
    "    return (labeled == largest).astype(np.uint8)\n",
    "\n",
    "def normalize_cc_size(mask, target_area):\n",
    "    \"\"\"Scale mask so its white-pixel area ≈ target_area.\"\"\"\n",
    "    current = mask.sum()\n",
    "    if current in (0, target_area):\n",
    "        return mask\n",
    "    scale = np.sqrt(target_area / current)\n",
    "    h, w   = mask.shape\n",
    "    new_h = max(1, int(round(h * scale)))\n",
    "    new_w = max(1, int(round(w * scale)))\n",
    "    pil = Image.fromarray((mask * 255).astype(np.uint8))\n",
    "    resized = pil.resize((new_w, new_h), resample=Image.BILINEAR)\n",
    "    return (np.array(resized) > 127).astype(np.uint8)\n",
    "\n",
    "def crop_to_shape(mask):\n",
    "    \"\"\"Crop mask tightly around its white pixels.\"\"\"\n",
    "    coords = np.argwhere(mask > 0)\n",
    "    if coords.size == 0:\n",
    "        return mask\n",
    "    r0, c0 = coords.min(axis=0)\n",
    "    r1, c1 = coords.max(axis=0)\n",
    "    return mask[r0:r1+1, c0:c1+1]\n",
    "\n",
    "def pad_image_to_square(img, target_length):\n",
    "    \"\"\"Pad image to a centered square of side target_length.\"\"\"\n",
    "    h, w = img.shape\n",
    "    pad_h = (target_length - h) // 2\n",
    "    pad_w = (target_length - w) // 2\n",
    "    out = np.zeros((target_length, target_length), dtype=np.uint8)\n",
    "    out[pad_h:pad_h+h, pad_w:pad_w+w] = img\n",
    "    return out\n",
    "\n",
    "# ─── ECT Extraction Class ─────────────────────────────────────────────────────\n",
    "class EctImg:\n",
    "    def __init__(self, nm, img, k=k, xinterval=xinterval, xpoints=xpoints):\n",
    "        self.nm    = nm\n",
    "        self.image = self.compute(img, k, xinterval, xpoints)\n",
    "\n",
    "    def compute(self, img, k, xinterval, xpoints):\n",
    "        cplx = ec.EmbeddedComplex(img)\n",
    "        cplx.preproc_ect()\n",
    "        thetas = np.linspace(0, 2 * np.pi, k, endpoint=False)\n",
    "        ect1   = np.empty((k, xpoints), dtype=float)\n",
    "        T      = np.linspace(xinterval[0], xinterval[1], xpoints)\n",
    "        for i, theta in enumerate(thetas):\n",
    "            direction = np.array((np.sin(theta), np.cos(theta)))\n",
    "            ect_dir   = cplx.compute_euler_characteristic_transform(direction)\n",
    "            ect1[i]   = [ect_dir.evaluate(t) for t in T]\n",
    "        return ect1\n",
    "\n",
    "# ─── New Curve‐Based Distance ────────────────────────────────────────────────\n",
    "def curve_distance(emp1, emp2, delta_x):\n",
    "    \"\"\"\n",
    "    emp1, emp2: (k, xpoints) arrays.\n",
    "    Compute L1 distance per row (∑|emp1[i] - emp2[i]| * delta_x), then take the maximum.\n",
    "    \"\"\"\n",
    "    row_l1 = np.sum(np.abs(emp1 - emp2), axis=1) * delta_x\n",
    "    return np.max(row_l1)\n",
    "\n",
    "def compute_distance_matrix_curve(ects, delta_x, n_jobs=-1, verbose=5):\n",
    "    def rowfun(i):\n",
    "        return i, [\n",
    "            curve_distance(ects[i].image, ects[j].image, delta_x)\n",
    "            for j in range(len(ects))\n",
    "        ]\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=verbose)(\n",
    "        delayed(rowfun)(i) for i in range(len(ects))\n",
    "    )\n",
    "    N = len(ects)\n",
    "    D = np.zeros((N, N), dtype=float)\n",
    "    for i, row in results:\n",
    "        D[i, :] = row\n",
    "    return D\n",
    "\n",
    "# ─── Main Pipeline ────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    # 1) List & filter GIF files by keyword\n",
    "    all_gifs = [f for f in os.listdir(datafolder) if f.lower().endswith('.gif')]\n",
    "    files    = [f for f in all_gifs if any(kw in f.lower() for kw in keywords)]\n",
    "    if not files:\n",
    "        raise RuntimeError(\"No matching GIFs found.\")\n",
    "\n",
    "    # 2) Assign labels by first matching keyword\n",
    "    labels = [ next(kw for kw in keywords if kw in f.lower()) for f in files ]\n",
    "\n",
    "    # 3) Load & CC‐filter masks in parallel\n",
    "    def load_mask(f):\n",
    "        img = Image.open(os.path.join(datafolder, f)).convert('L')\n",
    "        return filter_to_largest_cc(np.array(img))\n",
    "    masks = Parallel(n_jobs=-1, verbose=5)(\n",
    "        delayed(load_mask)(f) for f in files\n",
    "    )\n",
    "    areas = [m.sum() for m in masks]\n",
    "    target_area = max(areas)\n",
    "\n",
    "    # 4) Normalize size & crop\n",
    "    cropped = [\n",
    "        crop_to_shape(normalize_cc_size(m, target_area))\n",
    "        for m in masks\n",
    "    ]\n",
    "\n",
    "    # 5) Pad to square\n",
    "    dims         = [max(c.shape) for c in cropped]\n",
    "    target_length = max(dims) + padding_extra\n",
    "    padded       = {\n",
    "        files[i]: pad_image_to_square(cropped[i], target_length)\n",
    "        for i in range(len(files))\n",
    "    }\n",
    "\n",
    "    # 6) Compute ECT signatures\n",
    "    ects = [EctImg(f, padded[f]) for f in files]\n",
    "\n",
    "    # 7) Compute delta_x from the sampling grid\n",
    "    delta_x = (xinterval[1] - xinterval[0]) / (xpoints - 1)\n",
    "\n",
    "    # 8) Compute distance matrix with the new curve‐based metric\n",
    "    D = compute_distance_matrix_curve(ects, delta_x, n_jobs=-1, verbose=5)\n",
    "\n",
    "    # 9) Build RBF‐style kernel and classify with an SVM\n",
    "    gamma = 1.0 / np.median(D[np.triu_indices_from(D, k=1)])\n",
    "    K     = np.exp(-gamma * D)\n",
    "\n",
    "    idx        = np.arange(len(files))\n",
    "    accuracies = []\n",
    "    for _ in range(n_repeats):\n",
    "        ti, te, ytr, yte = train_test_split(\n",
    "            idx, labels, test_size=test_frac, stratify=labels\n",
    "        )\n",
    "        Ktr = K[np.ix_(ti, ti)]\n",
    "        Kte = K[np.ix_(te, ti)]\n",
    "\n",
    "        clf = SVC(kernel='precomputed')\n",
    "        clf.fit(Ktr, ytr)\n",
    "        accuracies.append(clf.score(Kte, yte))\n",
    "\n",
    "    print(f\"SVM accuracy over {n_repeats} runs: \"\n",
    "          f\"{np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 397 out of 420 | elapsed:    1.4s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 420 out of 420 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    3.1s\n",
      "/Users/hyang/Library/Python/3.9/lib/python/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done 420 out of 420 | elapsed:   23.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy over 100 runs: 0.5102 ± 0.0188\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.ndimage import label\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import eucalc as ec  # ensure 'eucalc_directory' is on PYTHONPATH\n",
    "import numba\n",
    "from numba.typed import List as NBList\n",
    "from skimage import measure\n",
    "\n",
    "# ─── Settings ─────────────────────────────────────────────────────────────────\n",
    "datafolder    = \".\"       # where your .gif files live\n",
    "k             = 360       # number of directions for SECT\n",
    "xinterval     = (-1.5, 1.5)\n",
    "xpoints       = 12000     # sample points along each direction\n",
    "padding_extra = 10        # extra pixels when padding to square\n",
    "keywords      = [\n",
    "    'apple','bell','bottle','car','classic',\n",
    "    'cup','device','face','heart','key'\n",
    "]\n",
    "n_repeats     = 100       # how many train/test splits\n",
    "test_frac     = 0.30      # fraction for test set (70/30 split)\n",
    "\n",
    "# ─── Preprocessing ────────────────────────────────────────────────────────────\n",
    "def filter_to_largest_cc(img_array):\n",
    "    mask = img_array > 0\n",
    "    labeled, num = label(mask, structure=np.ones((3,3)))\n",
    "    if num < 1:\n",
    "        return np.zeros_like(img_array, dtype=np.uint8)\n",
    "    sizes  = np.bincount(labeled.ravel())\n",
    "    largest = np.argmax(sizes[1:]) + 1\n",
    "    return (labeled == largest).astype(np.uint8)\n",
    "\n",
    "def normalize_cc_size(mask, target_area):\n",
    "    cur = mask.sum()\n",
    "    if cur in (0, target_area):\n",
    "        return mask\n",
    "    scale = np.sqrt(target_area / cur)\n",
    "    h,w   = mask.shape\n",
    "    new_h = max(1, int(round(h*scale)))\n",
    "    new_w = max(1, int(round(w*scale)))\n",
    "    pil = Image.fromarray((mask*255).astype(np.uint8))\n",
    "    resized = pil.resize((new_w,new_h), resample=Image.BILINEAR)\n",
    "    return (np.array(resized)>127).astype(np.uint8)\n",
    "\n",
    "def crop_to_shape(mask):\n",
    "    coords = np.argwhere(mask>0)\n",
    "    if coords.size==0:\n",
    "        return mask\n",
    "    r0,c0 = coords.min(axis=0)\n",
    "    r1,c1 = coords.max(axis=0)\n",
    "    return mask[r0:r1+1, c0:c1+1]\n",
    "\n",
    "def pad_image_to_square(img, target_length):\n",
    "    h,w   = img.shape\n",
    "    pad_h = (target_length-h)//2\n",
    "    pad_w = (target_length-w)//2\n",
    "    out   = np.zeros((target_length,target_length), dtype=np.uint8)\n",
    "    out[pad_h:pad_h+h, pad_w:pad_w+w] = img\n",
    "    return out\n",
    "\n",
    "# ─── Contours → Simplicial ────────────────────────────────────────────────────\n",
    "def extract_boundary(mask):\n",
    "    return measure.find_contours(mask.astype(float), level=0.5)\n",
    "\n",
    "def contours_to_simplicial(contours):\n",
    "    all_pts, simplices = [], []\n",
    "    offset = 0\n",
    "    for loop in contours:\n",
    "        N = loop.shape[0]\n",
    "        all_pts.append(loop)\n",
    "        for j in range(N):\n",
    "            v1 = offset + j     + 1\n",
    "            v2 = offset + (j+1)%N + 1\n",
    "            simplices.append([v1, v2])\n",
    "        offset += N\n",
    "    data = np.vstack(all_pts)\n",
    "    return data, simplices\n",
    "\n",
    "# ─── Numba‐njitted SECT internals ─────────────────────────────────────────────\n",
    "@numba.njit()\n",
    "def euler_critical_values(simp_comp, data, direction):\n",
    "    n = len(simp_comp)\n",
    "    filt = np.empty((n,2), dtype=np.float64)\n",
    "    for i in range(n):\n",
    "        simplex = simp_comp[i]\n",
    "        fv = -1e18\n",
    "        for j in range(len(simplex)):\n",
    "            v = simplex[j]\n",
    "            dv = data[v-1][0]*direction[0] + data[v-1][1]*direction[1]\n",
    "            if dv > fv:\n",
    "                fv = dv\n",
    "        filt[i,0] = float(len(simplex))\n",
    "        filt[i,1] = fv\n",
    "    return filt\n",
    "\n",
    "@numba.njit()\n",
    "def euler_curve(simp_comp, data, direction, interval, points):\n",
    "    filt = euler_critical_values(simp_comp, data, direction)\n",
    "    idx  = np.argsort(filt[:,1])\n",
    "    filt = filt[idx]\n",
    "    step = (interval[1]-interval[0])/(points-1)\n",
    "    chi  = np.empty(points, dtype=np.float64)\n",
    "    val  = 0.0\n",
    "    c    = 0\n",
    "    for i in range(points):\n",
    "        x = interval[0] + i*step\n",
    "        while c < filt.shape[0] and filt[c,1] <= x + step:\n",
    "            dim = int(filt[c,0])\n",
    "            val += (-1)**(dim-1)\n",
    "            c   += 1\n",
    "        chi[i] = val\n",
    "    return chi\n",
    "\n",
    "@numba.njit()\n",
    "def cumulative_euler_curve(simp_comp, data, direction,\n",
    "                            interval, points, factor):\n",
    "    ec = euler_curve(simp_comp, data, direction,\n",
    "                     interval, points*factor)\n",
    "    mean = np.mean(ec)\n",
    "    step = (interval[1]-interval[0])/(points*factor - 1)\n",
    "    cec  = np.cumsum(ec)[::factor]*step - \\\n",
    "           np.linspace(0, interval[1]-interval[0], points)*mean\n",
    "    return cec\n",
    "\n",
    "@numba.njit(parallel=True)\n",
    "def _sect_2d(simp_comp, data, k, interval, points, factor):\n",
    "    sect = np.empty((k, points), dtype=np.float64)\n",
    "    thetas = np.linspace(0, 2*np.pi, k+1)\n",
    "    for i in numba.prange(k):\n",
    "        theta = thetas[i]\n",
    "        direction = np.array((np.sin(theta), np.cos(theta)),\n",
    "                             dtype=np.float64)\n",
    "        sect[i] = cumulative_euler_curve(simp_comp, data,\n",
    "                                         direction, interval,\n",
    "                                         points, factor)\n",
    "    return sect\n",
    "\n",
    "def sect_2d(simp_comp, data, k=20, interval=(-1.,1.),\n",
    "            points=100, mode='mean', factor=3):\n",
    "    # pack simplices\n",
    "    numba_simp = NBList()\n",
    "    for s in simp_comp:\n",
    "        numba_simp.append(NBList(s))\n",
    "    # pack data\n",
    "    numba_data = NBList()\n",
    "    for pt in data:\n",
    "        numba_data.append(NBList((float(pt[0]), float(pt[1]))))\n",
    "    # compute full or mean\n",
    "    full = _sect_2d(numba_simp, numba_data, k, interval, points, factor)\n",
    "    return full if mode=='full' else full.mean(axis=0)\n",
    "\n",
    "# ─── Distance on 1D SECT curves ───────────────────────────────────────────────\n",
    "def sect_distance(c1, c2, delta_x):\n",
    "    \"\"\"L1 norm of two 1D curves (sum |c1-c2| * delta_x).\"\"\"\n",
    "    return np.sum(np.abs(c1 - c2)) * delta_x\n",
    "\n",
    "def compute_distance_matrix_sect(curves, delta_x, n_jobs=-1, verbose=5):\n",
    "    def rowfun(i):\n",
    "        return i, [sect_distance(curves[i], curves[j], delta_x)\n",
    "                   for j in range(len(curves))]\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=verbose)(\n",
    "        delayed(rowfun)(i) for i in range(len(curves))\n",
    "    )\n",
    "    N = len(curves)\n",
    "    D = np.zeros((N, N), dtype=float)\n",
    "    for i, row in results:\n",
    "        D[i, :] = row\n",
    "    return D\n",
    "\n",
    "# ─── Main Pipeline ────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    # 1) gather & filter GIFs\n",
    "    all_gifs = [f for f in os.listdir(datafolder) if f.lower().endswith('.gif')]\n",
    "    files    = [f for f in all_gifs if any(kw in f.lower() for kw in keywords)]\n",
    "    if not files:\n",
    "        raise RuntimeError(\"No matching GIFs found.\")\n",
    "\n",
    "    # 2) labels by keyword\n",
    "    labels = [next(kw for kw in keywords if kw in f.lower()) for f in files]\n",
    "\n",
    "    # 3) load & CC-filter\n",
    "    def load_mask(f):\n",
    "        img = Image.open(os.path.join(datafolder,f)).convert('L')\n",
    "        return filter_to_largest_cc(np.array(img))\n",
    "    masks = Parallel(n_jobs=-1, verbose=5)(\n",
    "        delayed(load_mask)(f) for f in files\n",
    "    )\n",
    "    areas = [m.sum() for m in masks]\n",
    "    target_area = max(areas)\n",
    "\n",
    "    # 4) normalize & crop\n",
    "    cropped = [crop_to_shape(normalize_cc_size(m, target_area)) for m in masks]\n",
    "\n",
    "    # 5) pad to square\n",
    "    dims          = [max(c.shape) for c in cropped]\n",
    "    target_length = max(dims) + padding_extra\n",
    "    padded        = {\n",
    "        files[i]: pad_image_to_square(cropped[i], target_length)\n",
    "        for i in range(len(files))\n",
    "    }\n",
    "\n",
    "    # 6) extract contours & build 1D SECT (mean mode)\n",
    "    curves = []\n",
    "    for f in files:\n",
    "        loops = extract_boundary(padded[f])\n",
    "        data, simplices = contours_to_simplicial(loops)\n",
    "        curve = sect_2d(simplices, data,\n",
    "                        k=k,\n",
    "                        interval=xinterval,\n",
    "                        points=xpoints,\n",
    "                        mode='mean',\n",
    "                        factor=3)\n",
    "        curves.append(curve)\n",
    "\n",
    "    # 7) compute delta_x\n",
    "    delta_x = (xinterval[1] - xinterval[0]) / (xpoints - 1)\n",
    "\n",
    "    # 8) distance matrix\n",
    "    D = compute_distance_matrix_sect(curves, delta_x, n_jobs=-1, verbose=5)\n",
    "\n",
    "    # 9) kernel + SVM\n",
    "    gamma = 1.0 / np.median(D[np.triu_indices_from(D, k=1)])\n",
    "    K     = np.exp(-gamma * D)\n",
    "\n",
    "    idx, accs = np.arange(len(files)), []\n",
    "    for _ in range(n_repeats):\n",
    "        ti, te, ytr, yte = train_test_split(\n",
    "            idx, labels, test_size=test_frac, stratify=labels\n",
    "        )\n",
    "        Ktr = K[np.ix_(ti, ti)]\n",
    "        Kte = K[np.ix_(te, ti)]\n",
    "        clf = SVC(kernel='precomputed')\n",
    "        clf.fit(Ktr, ytr)\n",
    "        accs.append(clf.score(Kte, yte))\n",
    "\n",
    "    print(f\"SVM accuracy over {n_repeats} runs: \"\n",
    "          f\"{np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EulerImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 397 out of 420 | elapsed:    1.5s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 420 out of 420 | elapsed:    1.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:   56.3s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 264 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 420 out of 420 | elapsed:  4.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy over 100 runs: 0.9439 ± 0.0201\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.ndimage import label\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import eucalc as ec  # make sure 'eucalc_directory' is on PYTHONPATH\n",
    "\n",
    "# ─── Settings ─────────────────────────────────────────────────────────────────\n",
    "datafolder    = \".\"       # directory containing your .gif files\n",
    "k             = 360         # number of directions for ECT image density\n",
    "xinterval     = (-1.5, 1.5)\n",
    "xpoints       = 12000        # sample points along each direction\n",
    "yinterval     = (0., 251)\n",
    "ypoints       = 251        # resolution of the density image\n",
    "padding_extra = 10         # extra pixels when padding to square\n",
    "keywords      = [\n",
    "    'apple','bell','bottle','car','classic',\n",
    "    'cup','device','face','heart','key'\n",
    "]\n",
    "n_repeats     = 100        # how many train/test splits to average over\n",
    "test_frac     = 0.30       # fraction for test set (70/30 split)\n",
    "\n",
    "# ─── Preprocessing Functions ──────────────────────────────────────────────────\n",
    "def filter_to_largest_cc(img_array):\n",
    "    \"\"\"Return binary mask of the largest connected white component.\"\"\"\n",
    "    mask = img_array > 0\n",
    "    labeled, num = label(mask, structure=np.ones((3,3)))\n",
    "    if num < 1:\n",
    "        return np.zeros_like(img_array, dtype=np.uint8)\n",
    "    sizes  = np.bincount(labeled.ravel())\n",
    "    largest = np.argmax(sizes[1:]) + 1\n",
    "    return (labeled == largest).astype(np.uint8)\n",
    "\n",
    "def normalize_cc_size(mask, target_area):\n",
    "    \"\"\"Scale mask so its white-pixel area ≈ target_area.\"\"\"\n",
    "    current = mask.sum()\n",
    "    if current in (0, target_area):\n",
    "        return mask\n",
    "    scale = np.sqrt(target_area / current)\n",
    "    h, w   = mask.shape\n",
    "    new_h = max(1, int(round(h * scale)))\n",
    "    new_w = max(1, int(round(w * scale)))\n",
    "    pil = Image.fromarray((mask * 255).astype(np.uint8))\n",
    "    resized = pil.resize((new_w, new_h), resample=Image.BILINEAR)\n",
    "    return (np.array(resized) > 127).astype(np.uint8)\n",
    "\n",
    "def crop_to_shape(mask):\n",
    "    \"\"\"Crop mask tightly around its white pixels.\"\"\"\n",
    "    coords = np.argwhere(mask > 0)\n",
    "    if coords.size == 0:\n",
    "        return mask\n",
    "    r0, c0 = coords.min(axis=0)\n",
    "    r1, c1 = coords.max(axis=0)\n",
    "    return mask[r0:r1+1, c0:c1+1]\n",
    "\n",
    "def pad_image_to_square(img, target_length):\n",
    "    \"\"\"Pad image to a centered square of side target_length.\"\"\"\n",
    "    h, w = img.shape\n",
    "    pad_h = (target_length - h) // 2\n",
    "    pad_w = (target_length - w) // 2\n",
    "    out = np.zeros((target_length, target_length), dtype=np.uint8)\n",
    "    out[pad_h:pad_h+h, pad_w:pad_w+w] = img\n",
    "    return out\n",
    "\n",
    "# ─── ECT Image‐Based Feature Extraction ───────────────────────────────────────\n",
    "class EctImg:\n",
    "    def __init__(self, nm, img,\n",
    "                 k=k,\n",
    "                 xinterval=xinterval, xpoints=xpoints,\n",
    "                 yinterval=yinterval, ypoints=ypoints):\n",
    "        self.nm        = nm\n",
    "        self.xinterval = xinterval\n",
    "        self.yinterval = yinterval\n",
    "        self.xpoints   = xpoints\n",
    "        self.ypoints   = ypoints\n",
    "        self.image     = self.compute(img, k,\n",
    "                                      xinterval, xpoints,\n",
    "                                      yinterval, ypoints)\n",
    "\n",
    "    def compute(self, img, k,\n",
    "                xinterval, xpoints,\n",
    "                yinterval, ypoints):\n",
    "        cplx   = ec.EmbeddedComplex(img)\n",
    "        cplx.preproc_ect()\n",
    "        thetas = np.linspace(0, 2*np.pi, k+1)\n",
    "        ect1   = np.empty((k, xpoints), dtype=float)\n",
    "        for i, theta in enumerate(thetas[:-1]):\n",
    "            direction   = np.array((np.sin(theta), np.cos(theta)))\n",
    "            ect_dir     = cplx.compute_euler_characteristic_transform(direction)\n",
    "            T           = np.linspace(xinterval[0], xinterval[1], xpoints)\n",
    "            ect1[i, :]  = [ect_dir.evaluate(t) for t in T]\n",
    "\n",
    "        image = np.zeros((ypoints, xpoints), dtype=float)\n",
    "        yvals  = np.linspace(yinterval[0], yinterval[1], ypoints+1)\n",
    "        for col in range(xpoints):\n",
    "            column = ect1[:, col]\n",
    "            for row in range(ypoints):\n",
    "                if row < ypoints-1:\n",
    "                    mask = (yvals[row] <= column) & (column < yvals[row+1])\n",
    "                else:\n",
    "                    mask = (yvals[row] <= column) & (column <= yvals[row+1])\n",
    "                image[row, col] = mask.sum() / k\n",
    "        return image\n",
    "\n",
    "# ─── Entry‐Wise L1 Distance ───────────────────────────────────────────────────\n",
    "def compute_distance_matrix_l1(ects, n_jobs=-1, verbose=5):\n",
    "    \"\"\"Compute entry-wise L1 distance between ECT images.\"\"\"\n",
    "    def rowfun(i):\n",
    "        return i, [\n",
    "            np.sum(np.abs(ects[i].image - ects[j].image))\n",
    "            for j in range(len(ects))\n",
    "        ]\n",
    "    results = Parallel(n_jobs=n_jobs, verbose=verbose)(\n",
    "        delayed(rowfun)(i) for i in range(len(ects))\n",
    "    )\n",
    "    N = len(ects)\n",
    "    D = np.zeros((N, N), dtype=float)\n",
    "    for i, row in results:\n",
    "        D[i, :] = row\n",
    "    return D\n",
    "\n",
    "# ─── Main Pipeline ────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    # 1) List & filter GIF files by keyword\n",
    "    all_gifs = [f for f in os.listdir(datafolder) if f.lower().endswith('.gif')]\n",
    "    files    = [f for f in all_gifs if any(kw in f.lower() for kw in keywords)]\n",
    "    if not files:\n",
    "        raise RuntimeError(\"No matching GIFs found.\")\n",
    "\n",
    "    # 2) Assign labels by first matching keyword\n",
    "    labels = [ next(kw for kw in keywords if kw in f.lower()) for f in files ]\n",
    "\n",
    "    # 3) Load & CC‐filter masks in parallel\n",
    "    def load_mask(f):\n",
    "        img = Image.open(os.path.join(datafolder, f)).convert('L')\n",
    "        return filter_to_largest_cc(np.array(img))\n",
    "    masks = Parallel(n_jobs=-1, verbose=5)(\n",
    "        delayed(load_mask)(f) for f in files\n",
    "    )\n",
    "    areas = [m.sum() for m in masks]\n",
    "    target_area = max(areas)\n",
    "\n",
    "    # 4) Normalize size & crop\n",
    "    cropped = [\n",
    "        crop_to_shape(normalize_cc_size(m, target_area))\n",
    "        for m in masks\n",
    "    ]\n",
    "\n",
    "    # 5) Pad to square\n",
    "    dims         = [max(c.shape) for c in cropped]\n",
    "    target_length = max(dims) + padding_extra\n",
    "    padded       = {\n",
    "        files[i]: pad_image_to_square(cropped[i], target_length)\n",
    "        for i in range(len(files))\n",
    "    }\n",
    "\n",
    "    # 6) Compute ECT image signatures\n",
    "    ects = [EctImg(f, padded[f]) for f in files]\n",
    "\n",
    "    # 7) Compute distance matrix using entry-wise L1\n",
    "    D = compute_distance_matrix_l1(ects, n_jobs=-1, verbose=5)\n",
    "\n",
    "    # 8) Build RBF‐style kernel and classify with an SVM\n",
    "    gamma = 1.0 / np.median(D[np.triu_indices_from(D, k=1)])\n",
    "    K     = np.exp(-gamma * D)\n",
    "\n",
    "    idx        = np.arange(len(files))\n",
    "    accuracies = []\n",
    "    for _ in range(n_repeats):\n",
    "        ti, te, ytr, yte = train_test_split(\n",
    "            idx, labels, test_size=test_frac, stratify=labels\n",
    "        )\n",
    "        Ktr = K[np.ix_(ti, ti)]\n",
    "        Kte = K[np.ix_(te, ti)]\n",
    "\n",
    "        clf = SVC(kernel='precomputed')\n",
    "        clf.fit(Ktr, ytr)\n",
    "        accuracies.append(clf.score(Kte, yte))\n",
    "\n",
    "    print(f\"SVM accuracy over {n_repeats} runs: \"\n",
    "          f\"{np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
